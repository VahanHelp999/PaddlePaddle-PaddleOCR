# Folder Structure Explained

This document explains **every folder** in PaddleOCR, why it exists, and what it contains.

## ğŸ“‚ Top-Level Directory Structure

```
PaddleOCR/
â”œâ”€â”€ ppocr/              # Core OCR library (models, training, data)
â”œâ”€â”€ tools/              # Training and inference scripts
â”œâ”€â”€ configs/            # Configuration files for all models
â”œâ”€â”€ deploy/             # Deployment solutions
â”œâ”€â”€ paddleocr/          # User-facing Python API/CLI
â”œâ”€â”€ benchmark/          # Performance benchmarking tools
â”œâ”€â”€ ppstructure/        # Document structure analysis
â”œâ”€â”€ doc/                # Documentation (legacy)
â”œâ”€â”€ docs/               # Documentation (main)
â”œâ”€â”€ test_tipc/          # Testing infrastructure
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ applications/       # Example applications
â”œâ”€â”€ StyleText/          # Style text generation
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ setup.py           # Package installation
```

---

## 1. ğŸ§  `ppocr/` - The Brain of PaddleOCR

**Purpose**: This is the **core library** containing all models, training logic, data processing, and utilities.

### Why it exists?
- Separates core functionality from user interfaces
- Reusable across different entry points (CLI, API, training scripts)
- Clean architecture following software engineering best practices

### Directory Structure

```
ppocr/
â”œâ”€â”€ modeling/          # Model architectures
â”œâ”€â”€ data/              # Data loading and augmentation
â”œâ”€â”€ losses/            # Loss functions
â”œâ”€â”€ metrics/           # Evaluation metrics
â”œâ”€â”€ postprocess/       # Post-processing algorithms
â”œâ”€â”€ optimizer/         # Optimizers and learning rate schedulers
â”œâ”€â”€ utils/             # Utilities (logging, visualization, etc.)
â””â”€â”€ ext_op/            # Custom operators
```

---

### 1.1 `ppocr/modeling/` - Model Architectures

**Purpose**: Contains ALL model implementations for detection, recognition, classification, table recognition, and KIE.

#### Structure

```
modeling/
â”œâ”€â”€ architectures/     # Model builders and base classes
â”‚   â”œâ”€â”€ base_model.py           # BaseModel orchestrates components
â”‚   â”œâ”€â”€ distillation_model.py  # Knowledge distillation wrapper
â”‚   â””â”€â”€ __init__.py             # build_model() factory
â”‚
â”œâ”€â”€ backbones/         # Feature extraction (39 implementations)
â”‚   â”œâ”€â”€ det_mobilenet_v3.py    # Lightweight detection backbone
â”‚   â”œâ”€â”€ det_resnet_vd.py       # ResNet for detection
â”‚   â”œâ”€â”€ rec_svtrnet.py         # SVTR for recognition
â”‚   â”œâ”€â”€ rec_pphgnetv2.py       # PP-HGNetV2 for recognition
â”‚   â”œâ”€â”€ table_master_resnet.py # Table recognition backbone
â”‚   â””â”€â”€ ... (30+ more)
â”‚
â”œâ”€â”€ necks/             # Feature refinement (15 implementations)
â”‚   â”œâ”€â”€ db_fpn.py              # Feature Pyramid Network for DB
â”‚   â”œâ”€â”€ rnn.py                 # RNN for sequence modeling
â”‚   â”œâ”€â”€ csp_pan.py             # CSP-PAN for detection
â”‚   â””â”€â”€ ... (12 more)
â”‚
â”œâ”€â”€ heads/             # Task-specific output heads (37 implementations)
â”‚   â”œâ”€â”€ det_db_head.py         # DB detection head
â”‚   â”œâ”€â”€ rec_ctc_head.py        # CTC recognition head
â”‚   â”œâ”€â”€ rec_att_head.py        # Attention recognition head
â”‚   â”œâ”€â”€ table_master_head.py   # Table structure head
â”‚   â””â”€â”€ ... (33 more)
â”‚
â””â”€â”€ transforms/        # Spatial transformations
    â””â”€â”€ tps.py                 # Thin Plate Spline transformation
```

#### Why This Structure?

**The Modular Pattern**: `Transform â†’ Backbone â†’ Neck â†’ Head`

1. **Transform** (Optional): Spatial transformation (e.g., straighten curved text)
2. **Backbone**: Extract features from images (CNN-based)
3. **Neck**: Refine and combine features (FPN, RNN, etc.)
4. **Head**: Task-specific predictions (detection boxes, recognized text, etc.)

**Benefits**:
- **Reusability**: One backbone can be used for multiple tasks
- **Experimentation**: Easy to swap components (try different backbones)
- **Clarity**: Each component has a single responsibility

**Example Configuration**:
```yaml
Architecture:
  model_type: det
  algorithm: DB
  Transform: null              # No transformation
  Backbone:
    name: MobileNetV3          # Feature extraction
    scale: 0.5
  Neck:
    name: DBFPN                # Feature refinement
    out_channels: 256
  Head:
    name: DBHead               # Detection output
    k: 50
```

---

### 1.2 `ppocr/data/` - Data Loading & Augmentation

**Purpose**: Load datasets, apply augmentations, and create batches for training.

```
data/
â”œâ”€â”€ __init__.py                # build_dataloader() factory
â”œâ”€â”€ simple_dataset.py          # Standard dataset (reads label files)
â”œâ”€â”€ lmdb_dataset.py            # LMDB format support (faster I/O)
â”œâ”€â”€ collate_fn.py              # Batch collation
â””â”€â”€ imaug/                     # Data augmentation
    â”œâ”€â”€ operators.py           # Basic ops (resize, normalize)
    â”œâ”€â”€ label_ops.py           # Label encoding/decoding (75KB)
    â”œâ”€â”€ rec_img_aug.py         # Recognition augmentations
    â”œâ”€â”€ randaugment.py         # RandAugment policy
    â”œâ”€â”€ copy_paste.py          # Copy-paste augmentation
    â””â”€â”€ ... (20+ augmentation modules)
```

#### Why Data Augmentation is Complex?

OCR has unique challenges:
- **Text rotation**: Real-world text is often tilted
- **Perspective distortion**: Photos of documents have perspective issues
- **Blur and noise**: Low-quality images
- **Varying fonts and sizes**: Must generalize across styles

**Common Augmentations**:
- Rotation, scaling, cropping
- Color jittering
- Gaussian blur, motion blur
- Copy-paste (paste text regions onto new backgrounds)
- RandAugment (automatic augmentation policy search)

---

### 1.3 `ppocr/losses/` - Loss Functions

**Purpose**: Define training objectives for different tasks.

```
losses/
â”œâ”€â”€ det_db_loss.py             # DB detection loss
â”œâ”€â”€ det_east_loss.py           # EAST detection loss
â”œâ”€â”€ rec_ctc_loss.py            # CTC recognition loss
â”œâ”€â”€ rec_att_loss.py            # Attention recognition loss
â”œâ”€â”€ table_att_loss.py          # Table attention loss
â”œâ”€â”€ distillation_loss.py       # Knowledge distillation (41KB)
â””â”€â”€ ... (44 loss implementations)
```

#### Why So Many Losses?

Different tasks need different loss functions:
- **Detection**: Segmentation loss (pixel-level text/non-text)
- **Recognition**: Sequence loss (CTC or attention-based)
- **Table**: Combined structure + content loss
- **Distillation**: Transfer knowledge from large model to small model

---

### 1.4 `ppocr/metrics/` - Evaluation Metrics

**Purpose**: Measure model performance during training and evaluation.

```
metrics/
â”œâ”€â”€ det_metric.py              # IoU, Precision, Recall, F1-score
â”œâ”€â”€ rec_metric.py              # Accuracy (character & word level)
â”œâ”€â”€ cls_metric.py              # Classification accuracy
â”œâ”€â”€ table_metric.py            # Table structure accuracy
â””â”€â”€ ... (14 metrics)
```

---

### 1.5 `ppocr/postprocess/` - Post-Processing

**Purpose**: Convert raw model outputs to usable results.

```
postprocess/
â”œâ”€â”€ db_postprocess.py          # Convert probability maps to boxes
â”œâ”€â”€ rec_postprocess.py         # Decode predictions to text (58KB)
â”œâ”€â”€ cls_postprocess.py         # Classification post-processing
â””â”€â”€ ... (17 post-processing modules)
```

#### Example: Detection Post-Processing

**Model Output**: Probability map (each pixel = text likelihood)
```
[0.1, 0.9, 0.9, 0.1]
[0.1, 0.9, 0.9, 0.1]  â†’ Post-process â†’ Bounding box: [(1,0), (2,0), (2,1), (1,1)]
[0.1, 0.1, 0.1, 0.1]
```

#### Example: Recognition Post-Processing

**Model Output**: Sequence of character probabilities
```
[0.8: 'H', 0.1: 'A', ...]
[0.9: 'e', 0.05: 'o', ...]  â†’ Post-process â†’ "Hello"
[0.7: 'l', 0.2: 'i', ...]
```

---

### 1.6 `ppocr/optimizer/` - Training Optimization

**Purpose**: Learning rate schedulers and optimizer configurations.

```
optimizer/
â”œâ”€â”€ lr_scheduler.py            # Cosine, Step, Warmup, etc.
â”œâ”€â”€ optimizer.py               # Adam, SGD, etc.
â””â”€â”€ regularizer.py             # L1, L2 regularization
```

---

### 1.7 `ppocr/utils/` - Utilities

**Purpose**: Helper functions for logging, checkpointing, visualization, etc.

```
utils/
â”œâ”€â”€ dict/                      # Character dictionaries (80+ languages)
â”‚   â”œâ”€â”€ en_dict.txt           # English characters
â”‚   â”œâ”€â”€ ch_dict.txt           # Chinese characters
â”‚   â”œâ”€â”€ arabic_dict.txt       # Arabic characters
â”‚   â””â”€â”€ ... (80+ language dicts)
â”‚
â”œâ”€â”€ loggers/                   # Logging integrations
â”‚   â”œâ”€â”€ vdl_logger.py         # VisualDL logger
â”‚   â””â”€â”€ wandb_logger.py       # Weights & Biases logger
â”‚
â”œâ”€â”€ save_load.py              # Model checkpointing
â”œâ”€â”€ stats.py                  # Training statistics
â””â”€â”€ utility.py                # General utilities
```

---

## 2. ğŸ› ï¸ `tools/` - Training & Inference Scripts

**Purpose**: **Entry points** for training, evaluation, and inference.

### Why Separate from `ppocr/`?

- `ppocr/` = Library (reusable code)
- `tools/` = Scripts (executable programs)

### Structure

```
tools/
â”œâ”€â”€ train.py                   # Main training script (10KB)
â”œâ”€â”€ eval.py                    # Evaluation script
â”œâ”€â”€ export_model.py            # Export to inference format
â”œâ”€â”€ program.py                 # Training loop implementation (34KB)
â”‚
â”œâ”€â”€ infer_det.py              # Detection inference
â”œâ”€â”€ infer_rec.py              # Recognition inference
â”œâ”€â”€ infer_cls.py              # Classification inference
â”œâ”€â”€ infer_e2e.py              # End-to-end inference
â”œâ”€â”€ infer_table.py            # Table recognition
â”‚
â””â”€â”€ infer/                    # Prediction utilities
    â”œâ”€â”€ predict_det.py        # Detection predictor class
    â”œâ”€â”€ predict_rec.py        # Recognition predictor class
    â”œâ”€â”€ predict_cls.py        # Classification predictor class
    â”œâ”€â”€ predict_system.py     # Complete OCR system
    â””â”€â”€ utility.py            # Helper functions
```

### Key Scripts

#### `train.py` - Start Training
```bash
python tools/train.py -c configs/det/det_db.yml
```
- Loads config
- Builds dataloader, model, loss, optimizer
- Calls training loop

#### `eval.py` - Evaluate Model
```bash
python tools/eval.py -c configs/det/det_db.yml -o Global.checkpoints=output/model
```

#### `export_model.py` - Export for Deployment
```bash
python tools/export_model.py -c configs/det/det_db.yml -o Global.checkpoints=output/model
```
- Converts training model to inference model
- Removes training-specific layers

#### `infer_det.py` / `infer_rec.py` - Quick Inference
```bash
python tools/infer_det.py --image_dir=test.jpg
python tools/infer_rec.py --image_dir=text_crop.jpg
```

---

## 3. âš™ï¸ `configs/` - Configuration Files

**Purpose**: Define models, training settings, and hyperparameters using YAML.

### Why YAML Configs?

**Without configs** (hardcoded):
```python
model = DBNet(backbone='MobileNetV3', channels=96, scale=0.5)
```
- Requires code changes for experiments
- Hard to track what settings were used

**With configs**:
```yaml
Architecture:
  Backbone:
    name: MobileNetV3
    scale: 0.5
```
- Change settings without touching code
- Easy to version control experiments
- Reproducible research

### Structure

```
configs/
â”œâ”€â”€ det/                       # Text Detection configs
â”‚   â”œâ”€â”€ PP-OCRv3/
â”‚   â”œâ”€â”€ PP-OCRv4/
â”‚   â”œâ”€â”€ PP-OCRv5/
â”‚   â””â”€â”€ det_*.yml             # Other algorithms
â”‚
â”œâ”€â”€ rec/                       # Text Recognition configs
â”‚   â”œâ”€â”€ PP-OCRv3/
â”‚   â”œâ”€â”€ PP-OCRv4/
â”‚   â”œâ”€â”€ PP-OCRv5/
â”‚   â”œâ”€â”€ SVTRv2/
â”‚   â”œâ”€â”€ multi_language/       # 80+ languages
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ cls/                       # Text Angle Classification
â”œâ”€â”€ table/                     # Table Recognition
â”œâ”€â”€ kie/                       # Key Information Extraction
â””â”€â”€ e2e/                       # End-to-End OCR
```

### Config Sections

Every config has these sections:
1. **Global**: Training settings (epochs, save path, GPU, etc.)
2. **Architecture**: Model definition (backbone, neck, head)
3. **Loss**: Loss function
4. **Optimizer**: Learning rate, optimizer type
5. **PostProcess**: Post-processing settings
6. **Metric**: Evaluation metric
7. **Train**: Training data configuration
8. **Eval**: Evaluation data configuration

See `07_Configuration_System.md` for details.

---

## 4. ğŸš€ `deploy/` - Deployment Solutions

**Purpose**: Deploy models to production environments.

### Structure

```
deploy/
â”œâ”€â”€ cpp_infer/                # C++ inference engine
â”‚   â”œâ”€â”€ src/                  # C++ source
â”‚   â”œâ”€â”€ include/              # Headers
â”‚   â””â”€â”€ tools/                # Build scripts
â”‚
â”œâ”€â”€ android_demo/             # Android app
â”œâ”€â”€ ios_demo/                 # iOS app
â”œâ”€â”€ lite/                     # Paddle Lite (mobile/embedded)
â”‚
â”œâ”€â”€ hubserving/               # PaddleHub HTTP serving
â”‚   â”œâ”€â”€ ocr_det/             # Detection service
â”‚   â”œâ”€â”€ ocr_rec/             # Recognition service
â”‚   â”œâ”€â”€ ocr_system/          # Complete OCR service
â”‚   â””â”€â”€ structure_*/         # Structure services
â”‚
â”œâ”€â”€ paddle2onnx/             # ONNX export
â”œâ”€â”€ slim/                    # Model compression
â”‚   â”œâ”€â”€ quantization/        # Quantization (INT8)
â”‚   â”œâ”€â”€ prune/               # Pruning (remove weights)
â”‚   â””â”€â”€ auto_compression/    # Automatic compression
â”‚
â””â”€â”€ docker/                  # Docker deployment
```

### Why Multiple Deployment Options?

Different scenarios need different solutions:

| Deployment | Use Case | Language | Platform |
|------------|----------|----------|----------|
| Python API | Prototyping, Jupyter | Python | Any |
| C++ Inference | High performance | C++ | Server |
| Mobile (Lite) | Smartphones | Java/Swift | iOS/Android |
| ONNX | Cross-platform | Any | Any runtime |
| Docker | Cloud deployment | Any | Kubernetes |
| Hub Serving | HTTP API | Python | Server |

---

## 5. ğŸ¯ `paddleocr/` - User-Facing API

**Purpose**: Simple Python interface for end users.

### Structure

```
paddleocr/
â”œâ”€â”€ __init__.py              # Main PaddleOCR class
â”œâ”€â”€ __main__.py              # CLI entry point
â”œâ”€â”€ paddleocr.py             # Core implementation
â””â”€â”€ tools/                   # Model download utilities
```

### Why Separate from `ppocr/`?

- `ppocr/`: Low-level library (for training & development)
- `paddleocr/`: High-level API (for users)

### Usage

```python
from paddleocr import PaddleOCR

# One-line initialization
ocr = PaddleOCR(use_angle_cls=True, lang='en')

# One-line inference
result = ocr.ocr('image.jpg')
```

**Behind the scenes**:
1. Downloads pre-trained models (first time only)
2. Loads detection, recognition, and classification models
3. Runs complete OCR pipeline
4. Returns structured results

---

## 6. ğŸ“Š `benchmark/` - Performance Benchmarking

**Purpose**: Measure and optimize model performance.

### Structure

```
benchmark/
â”œâ”€â”€ PaddleOCR_DBNet/         # DBNet benchmark
â”œâ”€â”€ analysis.py              # Performance analysis
â”œâ”€â”€ run_benchmark_det.sh     # Detection benchmark script
â””â”€â”€ run_det.sh              # Detection inference
```

### Why Benchmark?

To measure:
- **Inference speed** (FPS, latency)
- **Memory usage**
- **Accuracy** vs speed trade-offs
- **Optimization effects** (quantization, pruning)

### When to Use?

- Comparing different models
- Optimizing for production
- Hardware-specific tuning (CPU vs GPU)

---

## 7. ğŸ“„ `ppstructure/` - Document Structure Analysis

**Purpose**: Advanced document understanding (beyond basic OCR).

### Structure

```
ppstructure/
â”œâ”€â”€ layout/                   # Layout analysis (find regions)
â”œâ”€â”€ table/                    # Table recognition
â”‚   â”œâ”€â”€ table_metric/        # Table evaluation
â”‚   â””â”€â”€ tablepyxl/          # Excel export
â”œâ”€â”€ kie/                     # Key Information Extraction
â”œâ”€â”€ recovery/                # Document recovery (to Word/Markdown)
â””â”€â”€ pdf2word/                # PDF to Word conversion
```

### Why Separate?

- **PaddleOCR**: Basic OCR (text detection + recognition)
- **PaddleStructure**: Advanced document understanding

Not everyone needs document structure analysis, so it's modular.

---

## 8. ğŸ“š `doc/` and `docs/` - Documentation

**Purpose**: User guides, API references, tutorials.

```
doc/                         # Legacy documentation
docs/                        # Main documentation
â”œâ”€â”€ quick_start.md          # Getting started
â”œâ”€â”€ training.md             # Training guide
â”œâ”€â”€ inference.md            # Inference guide
â””â”€â”€ ...
```

---

## 9. âœ… `test_tipc/` and `tests/` - Testing

**Purpose**: Ensure code quality and catch bugs.

```
test_tipc/                   # Test in Production CI
tests/                       # Unit tests
```

---

## 10. ğŸ¨ `StyleText/` - Styled Text Generation

**Purpose**: Generate synthetic training data with various text styles.

### Why?

Training OCR models requires **lots of labeled data**. StyleText can:
- Generate realistic text images
- Apply different fonts, colors, backgrounds
- Create augmented training data

---

## 11. ğŸ’¼ `applications/` - Example Applications

**Purpose**: Real-world use case examples.

Examples might include:
- Invoice processing
- ID card recognition
- License plate recognition

---

## Summary Table

| Folder | Purpose | For Training? | For Inference? | For Users? |
|--------|---------|---------------|----------------|------------|
| `ppocr/` | Core library | âœ… Yes | âœ… Yes | âŒ No (low-level) |
| `tools/` | Scripts | âœ… Yes | âœ… Yes | âš ï¸ Advanced users |
| `configs/` | Model configs | âœ… Yes | âš ï¸ Some | âš ï¸ Advanced users |
| `deploy/` | Deployment | âŒ No | âœ… Yes | âœ… Yes |
| `paddleocr/` | Simple API | âŒ No | âœ… Yes | âœ… Yes (main interface) |
| `benchmark/` | Benchmarking | âŒ No | âš ï¸ Optimization | âš ï¸ Advanced users |
| `ppstructure/` | Doc structure | âš ï¸ Some | âœ… Yes | âœ… Yes (if needed) |
| `doc/docs/` | Documentation | âŒ No | âŒ No | âœ… Yes |
| `tests/` | Testing | âŒ No | âŒ No | âŒ No (developers) |

---

## Quick Reference

### I want to...

**Use PaddleOCR** â†’ Start with `paddleocr/`
**Train a model** â†’ Use `tools/train.py` + `configs/`
**Understand models** â†’ Read `ppocr/modeling/`
**Deploy to production** â†’ Check `deploy/`
**Benchmark performance** â†’ Use `benchmark/`
**Process documents** â†’ Explore `ppstructure/`

---

Next: [Architecture Deep Dive](./03_Architecture_Explained.md)
